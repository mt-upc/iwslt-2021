# @package _global_

common:
  seed: 4
  user_dir: ${env:IWSLT_ROOT}/fairseq_modules/
  tensorboard_logdir: ${env:SAVE_DIR}/lna_ed_adapt_2step_1_ca_en/tb_logs/
  fp16: True
  memory_efficient_fp16: True

task:
  _name: speech_to_text_iwslt21
  data: ${env:DATA_ROOT}
  max_source_positions: 400_000   # 25s @ 16kHz
  max_target_positions: 1024
  normalize: True
  da_p_augm: 0.8
  da_tempo: 0.85,1.3
  da_pitch: -300,300
  da_echo_delay: 20,200
  da_echo_decay: 0.05,0.2

distributed_training:
  find_unused_parameters: True

dataset:
  train_subset: train_covost
  valid_subset: dev_covost
  num_workers: 4
  max_tokens: 960_000             # 60s @ 16kHz
  batch_size: 36
  required_batch_size_multiple: 1
  max_tokens_valid: 960_000
  skip_invalid_size_inputs_valid_test: True

criterion:
  _name: label_smoothed_cross_entropy
  label_smoothing: 0.2
  ignore_prefix_size: 1

checkpoint:
  save_dir: ${env:SAVE_DIR}/lna_ed_adapt_2step_1_ca_en/ckpts/
  keep_best_checkpoints: 5

optimization:
  lr: [5e-04]
  max_update: 13500               # ~50 epochs
  update_freq: [8]
  sentence_avg: True
  clip_norm: 4.0

optimizer:
    _name: adam
    adam_betas: (0.9,0.98)
    adam_eps: 1e-08

lr_scheduler:
    _name: reduce_lr_on_plateau
    lr_shrink: 0.7

model:
  _name: wav2vec_seq2seq_iwslt21

  freeze_layers: encoder.feat_extr,encoder.self_attn,encoder.layer_norm,encoder.ffn,decoder.embedding,decoder.self_attn,decoder.layer_norm,decoder.encoder_attn,decoder.ffn

  load_pretrained_w2v_from_hf: ccoreilly/wav2vec2-large-100k-voxpopuli-catala
  load_pretrained_decoder_from: ${env:MBART_ROOT}/model.pt
  autoregressive: True
  feature_grad_mult: 0.0

  apply_mask: True
  mask_prob: 0.2
  mask_channel_prob: 0.1
  mask_channel_length: 64

  dropout: 0.0
  attention_dropout: 0.0
  activation_dropout: 0.0
  final_dropout: 0.1
  layerdrop: 0.0

  adapter_dim: 4096
  adapter_dropout: 0.1

  len_adaptor_kernel_sizes: 3,3,3
  len_adaptor_channels: 1024

  decoder_embed_dim: 1024
  decoder_output_dim: 1024
  decoder_ffn_embed_dim: 4096
  decoder_layers: 12
  decoder_attention_heads: 16
  decoder_learned_pos: True
  decoder_normalize_before: True
  decoder_dropout: 0.0
  decoder_attention_dropout: 0.0
  decoder_enc_attention_dropout: 0.0
  decoder_activation_dropout: 0.0
  share_decoder_input_output_embed: True
  max_target_positions: "${task.max_target_positions}"

hydra:
  run:
    dir: ${env:SAVE_DIR}/lna_ed_adapt_2step_1_ca_en/hydra_outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${env:SAVE_DIR}/lna_ed_adapt_2step_1_ca_en/hydra_multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
