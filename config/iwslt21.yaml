# @package _global_

common:
  seed: 1
  user_dir: ${env:IWSLT_ROOT}/fairseq_modules/
  tensorboard_logdir: ${env:SAVE_DIR}/${env:EXP_NAME}/tb_logs/
  fp16: True
  memory_efficient_fp16: True

task:
  _name: speech_to_text_iwslt21
  data: ${env:DATA_ROOT}
  max_source_positions: 400_000   # 25s @ 16kHz
  max_target_positions: 1024
  normalize: True
  da_p_augm: 0.8
  da_tempo: 0.85,1.3
  da_pitch: -300,300
  da_echo_delay: 20,200
  da_echo_decay: 0.05,0.2
  sample_ratios: 1,0.3,1,0.3,1

distributed_training:
  find_unused_parameters: True

dataset:
  train_subset: train_mustc,train_covost,train_europarlst,train_dev_covost,train_dev_europarlst
  valid_subset: dev_mustc
  num_workers: 4                  # n_cpu // 2
  max_tokens: 480_000             # 30s @ 16kHz
  batch_size: 18
  required_batch_size_multiple: 1
  max_tokens_valid: 960_000
  skip_invalid_size_inputs_valid_test: True

criterion:
  _name: label_smoothed_cross_entropy
  label_smoothing: 0.2
  ignore_prefix_size: 1

checkpoint:
  save_dir: ${env:SAVE_DIR}/${env:EXP_NAME}/ckpts/
  save_interval_updates: 500
  keep_interval_updates: 10
  keep_best_checkpoints: 5

optimization:
  lr: [8.75e-05]
  max_update: 23136 # 16 epochs x 1446 updates per epoch
  update_freq: [16]
  sentence_avg: True
  clip_norm: 10.0

optimizer:
    _name: adam
    adam_betas: (0.9,0.98)
    adam_eps: 1e-08

lr_scheduler:
    _name: tri_stage
    phase_ratio: [0.1, 0.2, 0.7]
    init_lr_scale: 0.01
    final_lr_scale: 0.01

model:
  _name: wav2vec_seq2seq_iwslt21
  w2v_path: ${env:WAV2VEC_ROOT}/wav2vec_vox_960h_pl.pt
  load_pretrained_decoder_from: ${env:MBART_ROOT}/model.pt
  autoregressive: True

  apply_mask: True
  mask_prob: 0.2
  mask_channel_prob: 0.1
  mask_channel_length: 64

  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.1

  feature_grad_mult: 0.0
  layerdrop: 0.1

  freeze_layers: encoder.feat_extr,encoder.ffn,decoder.embedding,decoder.self_attn,decoder.ffn
  # adapter_dim: 4096
  len_adaptor_kernel_sizes: 3,3,3
  len_adaptor_channels: 1024

  decoder_embed_dim: 1024
  decoder_output_dim: 1024
  decoder_ffn_embed_dim: 4096
  decoder_layers: 12
  decoder_attention_heads: 16
  decoder_learned_pos: True
  decoder_normalize_before: True
  decoder_dropout: 0.3
  decoder_attention_dropout: 0.1
  decoder_activation_dropout: 0.0
  share_decoder_input_output_embed: True
  max_target_positions: "${task.max_target_positions}"

hydra:
  run:
    dir: ${env:SAVE_DIR}/${env:EXP_NAME}/hydra_outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${env:SAVE_DIR}/${env:EXP_NAME}/hydra_multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
